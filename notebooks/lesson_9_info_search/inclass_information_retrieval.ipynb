{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информационный поиск\n",
    "\n",
    "**Ранжирование** — это сортировка каких-либо объектов из соображения их относительной релевантности («важности»).\n",
    "\n",
    "Где применяется:\n",
    "\n",
    "* Поиск по текстовым документам\n",
    "\n",
    "* Поиск по другим объектам: картинкам, видео, музыке, товарам, etc.\n",
    "\n",
    "* Рекомендательные и рейтинговые системы, коллаборативная фильтрация\n",
    "\n",
    "* Нахождение определений, вопросно-ответные системы\n",
    "\n",
    "* Оценка важности писем, фильтрация спама\n",
    "\n",
    "* Выбор лучшего варианта в машинном переводе и распознавании речи\n",
    "\n",
    "* Автодополнение в умных клавиатурах, подсказки в поиске\n",
    "\n",
    "* Поиск подходящего ответа в разговорных чат-ботах\n",
    "\n",
    "**Как измерять качество ранжирования.**\n",
    "\n",
    "У ранжируемых объектов есть какое-то число $r_i$, соответствующее «эталонной» релевантности. Существует два основных способа его получения:\n",
    "\n",
    "1. На основе исторических данных. Например, в случае рекомендаций контента, можно взять просмотры (клики, лайки, покупки) пользователя и присвоить просмотренным весам соответствующих элементов 1, а всем остальным — 0.  \n",
    "2. На основе экспертной оценки. Например, в задаче поиска, для каждого запроса можно привлечь команду асессоров, которые вручную оценят релевантности документов запросу. Обычно оценка категориальная (то есть не действительное число, а несколько градаций, скажем, 2, 3 или 5). \n",
    "\n",
    "Дальше возникает задача оценить качество ранжирования в целом, уже имея информацию о релевантности каждого элемента по отдельности. Условно, от идеального ранжирования мы хотим, чтобы объекты шли в порядке убывания релевантности, но есть несколько способов задать такие метрики.\n",
    "\n",
    "**Precision at k (p@k)** — «точность на $K$ элементах». Допустим, наш алгоритм ранжирования выдал оценки релевантности для каждого элемента. Отобрав среди них первые $k$ элементов с наибольшим $r_i$ посчитаем среди них долю релевантных — это и называется precision at k.\n",
    "\n",
    "**MRR** — mean reciprocal rank, дословно «средний обратный ранг».\n",
    "\n",
    "Эта метрика никак не учитывает абсолютные значения релевантности, а только их относительный порядок. Пусть $i$-тый самый релевантный элемент в нашем упорядочивании оказался на позиции $rank_i \\in [1, N]$. Тогда MRR считается как:\n",
    "\n",
    "$$\n",
    "MRR@k = \\frac{1}{K} \\sum_{i=1}^K \\frac{1}{rank_i}\n",
    "$$\n",
    "\n",
    "Чаще всего считают MRR@1, то есть просто по большому количеству запросов смотрят, на какой позиции находится самый подходящий документ, и усредняют обратные к этим позициям.\n",
    "\n",
    "**DCG** — discounted cumulative gain. Учитывает одновременно и значения релевантностей, и их порядок, путем домножения релевантности на вес, равный обратному логарифму номера позиции:\n",
    "\n",
    "$$\n",
    "DCG = \\sum_{i=1}^N \\frac{rel_i}{\\log_2 (i+1)}\n",
    "$$\n",
    "\n",
    "У неё есть проблема, связанная с тем, что, по-первых, оценки релевантности могут быть неотмасштабированными, и, во-вторых, на больших последовательностях она выше.\n",
    "\n",
    "Поэтому вводят $nDCG$ («нормализованный DCG» принимающий значения от 0 до 1), равный отношению DCG данного ранжирования к «идеальному», то есть максимально возможному (iDCG):\n",
    "\n",
    "$$\n",
    "nDCG = \\frac{DCG}{iDCG}\n",
    "$$\n",
    "\n",
    "$iDCG$ можно получить, отсортировав выдачу по реальным релевантностям и посчитав DCG на нём.\n",
    "\n",
    "**Информационным поиском** (*information retrieval*) называют процесс поиска неструктурированной информации в (большой) коллекции документов, а также некоторые оффтопные задачи, вроде кластеризации документов (склейки дубликатов), нахождения и фильтрации нежелательного контента, аннотирования, и т. д.\n",
    "\n",
    "Поиск это вообще не очень формально поставленная задача. В сервисах, где на его качество не пофиг, обычно даже разделяют команду на две: одна определяет, что такое хорошая выдача (то есть придумывает метрику), а вторая играется в каггл.\n",
    "\n",
    "Метрику обычно даже придумывают не сразу, а делают несколько аппроксимаций того, чего в конечном итоге хотят.\n",
    "\n",
    "* На верхнем уровне может быть что-то вроде количества пользователей (retention, деньги, цена акций etc.).\n",
    "\n",
    "* Чтобы её оптимизировать, пытаются посчитать условное «счастье пользователя» — по логам целой сессии определить её успешность.\n",
    "\n",
    "* Потом привлекают асессоров и с учетом оценок пытаются понять, как данное ранжирование результатов повлияло на счастье.\n",
    "\n",
    "* Потом из этой метрики делают уже что-то, что можно оптимизировать напрямую методами машинного обучения. \n",
    "\n",
    "Для поиска есть даже [отдельная мини-конференция](%5Bhttps://ru.wikipedia.org/wiki/%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B9%D1%81%D0%BA%D0%B8%D0%B9_%D1%81%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80_%D0%BF%D0%BE_%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%B2_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0%5D(https://ru.wikipedia.org/wiki/%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D0%B9%D1%81%D0%BA%D0%B8%D0%B9_%D1%81%D0%B5%D0%BC%D0%B8%D0%BD%D0%B0%D1%80_%D0%BF%D0%BE_%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B5_%D0%BC%D0%B5%D1%82%D0%BE%D0%B4%D0%BE%D0%B2_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BF%D0%BE%D0%B8%D1%81%D0%BA%D0%B0), на которой занимаются только метриками. Но в конечном итоге последняя, самая интересная часть, сводится к ранжированию.\n",
    "\n",
    "**pFound.** В Яндексе для поисковых задач когда-то придумали и [до сих пор повсеместно используют](https://catboost.ai/docs/references/pfound.html) метрику, которая называется pfound — грубо говоря, вероятность того, что пользователь найдёт на странице выдачи (среди первых $n$ документов) то, что искал.\n",
    "\n",
    "$$\n",
    "pfound = \\sum_{i=1}^n pLook(i) \\cdot pRel(i)\n",
    "$$\n",
    "\n",
    "Где $pLook$ определяется рекурсивно, как вероятность того, что пользователь вообще посмотрит на $i$-тый документ:\n",
    "\n",
    "$$\n",
    "pLook(i) = \\begin{cases}\n",
    "1, &i=1\\\\\n",
    "pLook(i-1) \\cdot (1 - pRel(i-1)), &i > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Интуиция такая: пользователь просматривает страницы выдачи сверху вниз, пока не найдёт релевантный документ, вероятность чего равна $pRel(i) = f(r_i)$, то есть как-то зависит от асессорской оценки релевантности.\n",
    "\n",
    "### Фичи в поиске\n",
    "\n",
    "Поисковики часто обвиняют в редактировании результатов, но на самом деле условный Google ещё лет 10 назад потерял понимание того, как работает поиск — потому что всё ранжирование начали делать машинным обучением.\n",
    "\n",
    "Это работает так: обучается модель, которая учится по паре запрос-документ выдавать одно действительное число — релевантность — по которой и сортируются результаты.\n",
    "\n",
    "В качестве модели может быть что угодно — нейросеть, линейные модели, решающие деревья на регрессию. Они принимают обычные фичи, какие-то из которых относятся только к запросу, какие-то — только к документу, какие-то — и к запросу, и к документу. Например:\n",
    "\n",
    "* Легко вычислимые текстовые фичи. Например, сколько слов из запроса встречаются в заголовке, в подзаголовках, во всем тексте.\n",
    "\n",
    "* Разреженный tf-idf вектор слов из запроса.\n",
    "\n",
    "* Разная статистика про документ: CTR по запросу, посещаемость сайта, PageRank, сколько в среднем пользователи проводят на этой странице, кликают ли они дальше на какие-то результаты, etc.\n",
    "\n",
    "* Информация о пользователе: возраст, пол, страна, увлечения, любимые сайты, etc.\n",
    "\n",
    "* Машинно-обучаемые фичи — например, можно обучить нейросеть, предсказываюзую релевантность по тексту запроса и тексту заголовка.\n",
    "\n",
    "Часто поиск бывает устроен иерархически — это связано с тем, что считать некоторые фичи и применять поверх них тяжелые модели долго и дорого. Этапов может быть 4-5, например — на первом рассматриваются десятки тысяч документов, а на последнем, например, упорядочивается только несколько десятков — то, что увидит пользователь.\n",
    "\n",
    "### Индексирование\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Какие страницы прокачивать (ресурсы нерезиновые), как часто и в каком формате хранить — отдельные задачи. После того, как данные получены, по ним строится какая-то структура (называемая **индексом**), позволяющая быстро какими-то жадными методами получить по запросу небольшое (не миллиарды, а несколько тысяч) количество документов, которые потом ранжируются чем-то нормальным.\n",
    "\n",
    "Для текстовых документов обычно строят **инвертированный индекс** — в нем хранится идентификатор слова и перечисление документов, в которых оно содержится. Чтобы списки не были слишком большими, их обычно в оффлайне сортируют по полезности пр фичам, не зависящим от запроса (например, по PageRank) и рассматривают только первые несколько тысяч документов.\n",
    "\n",
    "Очень сильно помогают индексированию новых документов логи из браузеров — поэтому почти все поисковые компанию делают либо полноценные браузеры, либо расширения типа яндексбара. Когда вы жмёте галочку «да, я хочу помочь Google / Microsoft / Yandex сделать мир лучше», содержимое посещенных вами страниц отправляется самим браузером напрямую поисковику. Летом 2018 года по этой причине приватные Google Docs начали появляться в выдаче Яндекса, а за несколько лет до этого внутренняя вики Яндекса попала в выдачу Google, после чего в компании запретили пользоваться Google Chrome.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module ''encodings''. Consider installing this module. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.\n",
      "<a href='https://aka.ms/kernelFailuresMissingModule'>Learn more</a>"
     ]
    }
   ],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module ''encodings''. Consider installing this module. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.\n",
      "<a href='https://aka.ms/kernelFailuresMissingModule'>Learn more</a>"
     ]
    }
   ],
   "source": [
    "a = 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
