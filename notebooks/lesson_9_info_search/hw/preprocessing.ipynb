{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import langid\n",
    "import re\n",
    "from concurrent import futures\n",
    "import swifter\n",
    "import asyncio\n",
    "import nltk\n",
    "# import vaex\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import  pos_tag\n",
    "from nltk import WordNetLemmatizer\n",
    "import modin.pandas as pdmd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_body.pickle\", 'rb') as f:\n",
    "    tfidf_body = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание масенького датасета из большого"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нашла я датасет гитхабовских проблем на 2гб"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"https://github.com/zhangyuanwei/node-images/i...</td>\n",
       "      <td>can't load the addon. issue to: https://github...</td>\n",
       "      <td>can't load the addon. issue to: https://github...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"https://github.com/Microsoft/pxt/issues/2543\"</td>\n",
       "      <td>hcl accessibility a11yblocking a11ymas mas4.2....</td>\n",
       "      <td>user experience: user who depends on screen re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1265: issue 1264: issue 1261: issue 1260...</td>\n",
       "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1266: issue 1263: issue 1262: issue 1259...</td>\n",
       "      <td>gitlo = github x trello\\n---\\nthis board is no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"https://github.com/MatisiekPL/Czekolada/issue...</td>\n",
       "      <td>issue 1288: issue 1285: issue 1284: issue 1281...</td>\n",
       "      <td>┆attachments: &lt;a href= https:&amp; x2f;&amp; x2f;githu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332148</th>\n",
       "      <td>\"https://github.com/bayborodin/ror-full-3/issu...</td>\n",
       "      <td>создать модуль instancecounter, содержащий сле...</td>\n",
       "      <td>методы класса: - instances, который возвращает...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332149</th>\n",
       "      <td>\"https://github.com/eclipse/paho.mqtt.java/iss...</td>\n",
       "      <td>at org.eclipse.paho.client.mqttv3.internal.cli...</td>\n",
       "      <td>- bug exists release version 1.1.0 master bran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332150</th>\n",
       "      <td>\"https://github.com/rzwitserloot/lombok/issues...</td>\n",
       "      <td>java.lang.linkageerror: loader constraint viol...</td>\n",
       "      <td>java.lang.linkageerror: loader constraint viol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332151</th>\n",
       "      <td>\"https://github.com/Gizra/productivity/issues/...</td>\n",
       "      <td>node : pdoexception: sqlstate 40001 : serializ...</td>\n",
       "      <td>view details in rollbar: https://rollbar.com/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332152</th>\n",
       "      <td>\"https://github.com/jacobmischka/coyote-grill/...</td>\n",
       "      <td>uncaught error: { error :{ errors : { domain :...</td>\n",
       "      <td>view details in rollbar: https://rollbar.com/j...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5332153 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 issue_url  \\\n",
       "0        \"https://github.com/zhangyuanwei/node-images/i...   \n",
       "1           \"https://github.com/Microsoft/pxt/issues/2543\"   \n",
       "2        \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "3        \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "4        \"https://github.com/MatisiekPL/Czekolada/issue...   \n",
       "...                                                    ...   \n",
       "5332148  \"https://github.com/bayborodin/ror-full-3/issu...   \n",
       "5332149  \"https://github.com/eclipse/paho.mqtt.java/iss...   \n",
       "5332150  \"https://github.com/rzwitserloot/lombok/issues...   \n",
       "5332151  \"https://github.com/Gizra/productivity/issues/...   \n",
       "5332152  \"https://github.com/jacobmischka/coyote-grill/...   \n",
       "\n",
       "                                               issue_title  \\\n",
       "0        can't load the addon. issue to: https://github...   \n",
       "1        hcl accessibility a11yblocking a11ymas mas4.2....   \n",
       "2        issue 1265: issue 1264: issue 1261: issue 1260...   \n",
       "3        issue 1266: issue 1263: issue 1262: issue 1259...   \n",
       "4        issue 1288: issue 1285: issue 1284: issue 1281...   \n",
       "...                                                    ...   \n",
       "5332148  создать модуль instancecounter, содержащий сле...   \n",
       "5332149  at org.eclipse.paho.client.mqttv3.internal.cli...   \n",
       "5332150  java.lang.linkageerror: loader constraint viol...   \n",
       "5332151  node : pdoexception: sqlstate 40001 : serializ...   \n",
       "5332152  uncaught error: { error :{ errors : { domain :...   \n",
       "\n",
       "                                                      body  \n",
       "0        can't load the addon. issue to: https://github...  \n",
       "1        user experience: user who depends on screen re...  \n",
       "2        ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
       "3        gitlo = github x trello\\n---\\nthis board is no...  \n",
       "4        ┆attachments: <a href= https:& x2f;& x2f;githu...  \n",
       "...                                                    ...  \n",
       "5332148  методы класса: - instances, который возвращает...  \n",
       "5332149  - bug exists release version 1.1.0 master bran...  \n",
       "5332150  java.lang.linkageerror: loader constraint viol...  \n",
       "5332151  view details in rollbar: https://rollbar.com/b...  \n",
       "5332152  view details in rollbar: https://rollbar.com/j...  \n",
       "\n",
       "[5332153 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"github_issues.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5332153</td>\n",
       "      <td>5332153</td>\n",
       "      <td>5332153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5315477</td>\n",
       "      <td>4862563</td>\n",
       "      <td>5041808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>\"https://github.com/cmty-test/cmty-repository-...</td>\n",
       "      <td>first from flow in uk south</td>\n",
       "      <td>first from flow in uk south</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>297</td>\n",
       "      <td>90133</td>\n",
       "      <td>90133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                issue_url  \\\n",
       "count                                             5332153   \n",
       "unique                                            5315477   \n",
       "top     \"https://github.com/cmty-test/cmty-repository-...   \n",
       "freq                                                  297   \n",
       "\n",
       "                        issue_title                         body  \n",
       "count                       5332153                      5332153  \n",
       "unique                      4862563                      5041808  \n",
       "top     first from flow in uk south  first from flow in uk south  \n",
       "freq                          90133                        90133  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_data = data.sample(frac=1, random_state=42)[:500_000]\n",
    "slice_data = slice_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_data.to_csv(\"github_issues_slice.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проведу мини-анализ по ускорению обработки данных\n",
    "\n",
    "Для начала мне нужно выделить из исходного датасета только текст на английском языке (мне так хочется). Для этой задачи уже можно попробовать ускорять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_en(text):\n",
    "    return text if langid.classify(text)[0] == 'en' else pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_en_md(text):\n",
    "    return text if langid.classify(text)[0] == 'en' else pdmd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "slice_data = pd.read_csv(\"github_issues_slice.csv\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "slice_data_md = pdmd.read_csv(\"github_issues_slice.csv\")[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply by pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9 s ± 181 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "res = slice_data.body.apply(is_en)\n",
    "res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.18 s ± 139 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "res = slice_data_md.body.apply(is_en_md)\n",
    "res.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    res = pd.Series(executor.map(is_en, slice_data.body))\n",
    "    res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "\n",
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    res = pdmd.Series(executor.map(is_en_md, slice_data_md.body))\n",
    "    res.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### future ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ProcessPoolExecutor_pd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProcessPoolExecutor_pd.py\n",
    "\n",
    "import pandas as pd\n",
    "import langid\n",
    "import time\n",
    "from concurrent import futures\n",
    "\n",
    "\n",
    "def is_en(text):\n",
    "    return text if langid.classify(text)[0] == 'en' else pd.NA\n",
    "\n",
    "def main(df):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    with futures.ProcessPoolExecutor() as executor:\n",
    "        res = pd.Series(executor.map(is_en, df.body))\n",
    "\n",
    "    print(len(res.dropna()))\n",
    "    print(time.perf_counter() - start)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(\"github_issues_slice.csv\")[:1000]\n",
    "\n",
    "    main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: Dask execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    from distributed import Client\n",
      "\n",
      "    client = Client()\n",
      "\n",
      "UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 53227 instead\n",
      "UserWarning: Distributing <class 'generator'> object. This may take some time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925\n",
      "24.9550099\n",
      "925\n",
      "19.636662700000002\n"
     ]
    }
   ],
   "source": [
    "!python ProcessPoolExecutor_pd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ProcessPoolExecutor_pdmd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ProcessPoolExecutor_pdmd.py\n",
    "\n",
    "import pandas as pd\n",
    "import modin.pandas as pdmd\n",
    "import langid\n",
    "import time\n",
    "from concurrent import futures\n",
    "\n",
    "\n",
    "def is_en(text):\n",
    "    return text if langid.classify(text)[0] == 'en' else pd.NA\n",
    "\n",
    "def main(df):\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    with futures.ProcessPoolExecutor() as executor:\n",
    "        res = pdmd.Series(executor.map(is_en, df.body))\n",
    "\n",
    "    print(len(res.dropna()))\n",
    "    print(time.perf_counter() - start)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pdmd.read_csv(\"github_issues_slice.csv\")[:1000]\n",
    "\n",
    "    main(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ProcessPoolExecutor_pdmd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1000/1000 [00:07<00:00, 133.94it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:09<00:00, 109.11it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:07<00:00, 136.19it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:07<00:00, 133.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:07<00:00, 141.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:07<00:00, 140.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:08<00:00, 113.86it/s]\n",
      "Pandas Apply: 100%|██████████| 1000/1000 [00:08<00:00, 111.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.88 s ± 885 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "res = slice_data.body.swifter.apply(is_en)\n",
    "res.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting asyncio_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile asyncio_1.py\n",
    "import pandas as pd\n",
    "import langid\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def is_en(df, text):\n",
    "    if langid.classify(text)[0] == 'en':\n",
    "        df.loc[df.body == text, 'body'] = text\n",
    "    else:\n",
    "        df.loc[df.body == text, 'body'] = pd.NA\n",
    "\n",
    "async def main(df):\n",
    "    start = time.perf_counter()\n",
    "    await asyncio.gather(*[is_en(df, text) for text in df.body])\n",
    "    print(len(df.dropna()))\n",
    "    print(time.perf_counter() - start)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(\"github_issues_slice.csv\")[:1000]\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main(df))\n",
    "    loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925\n",
      "17.1797964\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "!python asyncio_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = vaex.from_csv(\"github_issues_slice.csv\", convert=True, chunk_size=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = vaex.open('github_issues_slice.csv.hdf5')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_en(text):\n",
    "    return np.where(langid.classify(text)[0] == 'en', text, np.nan)\n",
    "    # return text if langid.classify(text)[0] == 'en' else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv.body = dv.body.apply(is_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "len(dv.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"github_issues_slice.csv\")[:10_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    data.body = pd.Series(executor.map(is_en, data.body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data.body.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"https://github.com/atais/angular-eonasdan-dat...</td>\n",
       "      <td>manually entered dates issues</td>\n",
       "      <td>i use format 'yyyy-mm-dd' option and bind ng-m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"https://github.com/conveyal/analysis-ui/issue...</td>\n",
       "      <td>highlight segment on map when editing speed</td>\n",
       "      <td>when editing the speed of a segment i.e. when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"https://github.com/Tat5ato/Phantasmic-Mind/is...</td>\n",
       "      <td>concept art for the otherworld</td>\n",
       "      <td>in general, the otherworld is craggy and organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"https://github.com/qmlweb/qmlweb/issues/420\"</td>\n",
       "      <td>mousearea and touch event in mobile browser</td>\n",
       "      <td>hello, in the master branch, mousearea doesn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"https://github.com/pybel/pybel/issues/174\"</td>\n",
       "      <td>function to drop graph store and edge store, b...</td>\n",
       "      <td>this function should go in the cache manager. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>\"https://github.com/nextcloud/serverinfo/issue...</td>\n",
       "      <td>settings button gone</td>\n",
       "      <td>when i deselected all kinds of logging entries...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>\"https://github.com/WhisperSystems/Signal-Andr...</td>\n",
       "      <td>using without gms</td>\n",
       "      <td>hello, one of my friends have some technical p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>\"https://github.com/automl/auto-sklearn/issues...</td>\n",
       "      <td>no module named 'configspace.io' during import...</td>\n",
       "      <td>i encounter this exception when trying to impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>\"https://github.com/gimli-org/gimli/issues/50\"</td>\n",
       "      <td>ship self-testing binaries</td>\n",
       "      <td>all distributed binaries of pygimli windows, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>\"https://github.com/brython-dev/brython/issues...</td>\n",
       "      <td>list aliasing correctness issue</td>\n",
       "      <td>python import copy a = 1 ,2,3 b = copy.copy a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9316 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              issue_url  \\\n",
       "0     \"https://github.com/atais/angular-eonasdan-dat...   \n",
       "1     \"https://github.com/conveyal/analysis-ui/issue...   \n",
       "2     \"https://github.com/Tat5ato/Phantasmic-Mind/is...   \n",
       "3         \"https://github.com/qmlweb/qmlweb/issues/420\"   \n",
       "4           \"https://github.com/pybel/pybel/issues/174\"   \n",
       "...                                                 ...   \n",
       "9994  \"https://github.com/nextcloud/serverinfo/issue...   \n",
       "9995  \"https://github.com/WhisperSystems/Signal-Andr...   \n",
       "9997  \"https://github.com/automl/auto-sklearn/issues...   \n",
       "9998     \"https://github.com/gimli-org/gimli/issues/50\"   \n",
       "9999  \"https://github.com/brython-dev/brython/issues...   \n",
       "\n",
       "                                            issue_title  \\\n",
       "0                         manually entered dates issues   \n",
       "1           highlight segment on map when editing speed   \n",
       "2                        concept art for the otherworld   \n",
       "3           mousearea and touch event in mobile browser   \n",
       "4     function to drop graph store and edge store, b...   \n",
       "...                                                 ...   \n",
       "9994                               settings button gone   \n",
       "9995                                  using without gms   \n",
       "9997  no module named 'configspace.io' during import...   \n",
       "9998                         ship self-testing binaries   \n",
       "9999                    list aliasing correctness issue   \n",
       "\n",
       "                                                   body  \n",
       "0     i use format 'yyyy-mm-dd' option and bind ng-m...  \n",
       "1     when editing the speed of a segment i.e. when ...  \n",
       "2     in general, the otherworld is craggy and organ...  \n",
       "3     hello, in the master branch, mousearea doesn't...  \n",
       "4     this function should go in the cache manager. ...  \n",
       "...                                                 ...  \n",
       "9994  when i deselected all kinds of logging entries...  \n",
       "9995  hello, one of my friends have some technical p...  \n",
       "9997  i encounter this exception when trying to impo...  \n",
       "9998  all distributed binaries of pygimli windows, c...  \n",
       "9999  python import copy a = 1 ,2,3 b = copy.copy a ...  \n",
       "\n",
       "[9316 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i use format 'yyyy-mm-dd' option and bind ng-model= formdata.date to my field. now if i enter '2015-03-05aaaaa', the date is displayed as '2015-03-05', but my $scope.formdata.date becomes '2015-03-05aaaaa'. so even if i write error message to user that date is invalid, in input field it looks like totally valid value '2015-03-05'. how do i stop datetimepicker from automatically correcting my dates? this misleads users.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\daris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\daris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_segmentation(text: str) -> list:\n",
    "    if re.match(r'[\\.!\\?;]', text[-1]):\n",
    "        text = text[:-1]\n",
    "    return re.split(r'[\\.!\\?;]\\s', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sentences: list) -> list:\n",
    "    return [re.split(r'[,:(\\s\\-)]*\\s', s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    my_switch = {\n",
    "        'J': wn.ADJ,\n",
    "        'V': wn.VERB,\n",
    "        'N': wn.NOUN,\n",
    "        'R': wn.ADV,\n",
    "    }\n",
    "    for key, item in my_switch.items():\n",
    "        if treebank_tag.startswith(key):\n",
    "            return item\n",
    "    return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(sentences: list) -> list:\n",
    "    sentences_tag  = [pos_tag(s) for s in sentences] # получаем теги слов каждого предложения\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemm_sentences = []\n",
    "    for sent in sentences_tag:\n",
    "        pos_tagged = [(word, get_wordnet_pos(tag)) for word, tag in sent]\n",
    "        lemm_sentences.append([lemmatizer.lemmatize(word, tag) for word, tag in pos_tagged])\n",
    "\n",
    "    return lemm_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')).union({'', ' '})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stopwords(sentences: list) -> list:\n",
    "    upd_sentences = []\n",
    "    re_sub = lambda x: re.sub(r\"[\\+=\\t\\r\\n,;:\\*'\\\"]+\",\"\", x)\n",
    "    union_sentences = lambda x: list(set().union(*x))\n",
    "\n",
    "    for sent in sentences:\n",
    "        upd_sentences.append([\n",
    "            re_sub(word) for word in sent if re_sub(word) not in stop_words and len(word) not in [1, 2]\n",
    "        ])\n",
    "    \n",
    "    return union_sentences(upd_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text: str) -> list:\n",
    "    return del_stopwords(lemmatization(tokenization(sentence_segmentation(text))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    data['words_body'] = pd.Series(executor.map(preprocessing_text, data.body))\n",
    "    data['words_title'] = pd.Series(executor.map(preprocessing_text, data.issue_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [03:05<00:00, 5381.33it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_vectors(fname, limit):\n",
    "  fin = io.open(fname, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n",
    "  n, d = map(int, fin.readline().split())\n",
    "  data = {}\n",
    "  for line in tqdm(islice(fin, limit), total = limit):\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "  return data\n",
    "\n",
    "vecs = load_vectors('crawl-300d-2M.vec', 1_000_000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(preprocessor=<function dummy_fun at 0x000001A93D5E28B0>,\n",
       "                token_pattern=None,\n",
       "                tokenizer=<function dummy_fun at 0x000001A93D5E28B0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_body = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)\n",
    "tfidf_body.fit(data.words_body)\n",
    "\n",
    "tfidf_title = TfidfVectorizer(analyzer='word', tokenizer=dummy_fun, preprocessor=dummy_fun, token_pattern=None)\n",
    "tfidf_title.fit(data.words_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization_body(words: list):\n",
    "    return tfidf_body.transform([words]).toarray().squeeze()\n",
    "\n",
    "def vectorization_title(words: list):\n",
    "    return tfidf_title.transform([words]).toarray().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with futures.ThreadPoolExecutor() as executor:\n",
    "    data['vectors_body'] = pd.Series(executor.map(vectorization_body, data.words_body))\n",
    "    data['vectors_title'] = pd.Series(executor.map(vectorization_title, data.words_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "zero = sum(vecs.values()) / len(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56719/56719 [00:03<00:00, 18649.21it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_body = np.zeros((len(tfidf_body.vocabulary_.keys()), dim))\n",
    "for key in tqdm(tfidf_body.vocabulary_.keys()):\n",
    "  vocab_body[tfidf_body.vocabulary_[key]] = vecs.get(key, zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11228/11228 [00:00<00:00, 92444.16it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab_title = np.zeros((len(tfidf_title.vocabulary_.keys()), dim))\n",
    "for key in tqdm(tfidf_title.vocabulary_.keys()):\n",
    "  vocab_title[tfidf_title.vocabulary_[key]] = vecs.get(key, zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vectors_body'] = np.array(data.vectors_body.tolist()).dot(vocab_body).tolist()\n",
    "data['vectors_title'] = np.array(data.vectors_title.tolist()).dot(vocab_title).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_pickle(obj, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(tfidf_title, \"tfidf_title.pickle\")\n",
    "save_pickle(tfidf_body, \"tfidf_body.pickle\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "919eb0904b72c37e8d6c7e3b2f7b6f162c89cafcc297fba09b2d10c79c52f5eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
