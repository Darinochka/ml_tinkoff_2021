{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:15<00:00, 6412.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "\n",
    "def load_vectors(fname, limit):\n",
    "  fin = io.open(fname, 'r', encoding = 'utf-8', newline = '\\n', errors = 'ignore')\n",
    "  n, d = map(int, fin.readline().split())\n",
    "  data = {}\n",
    "  for line in tqdm(islice(fin, limit), total = limit):\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "  return data\n",
    "\n",
    "vecs = load_vectors('crawl-300d-2M.vec', 100000)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Paris', 'France', 'Parisian', 'paris', 'Lyon', 'London', 'PARIS', 'French', 'Lille', 'Marseille', 'Toulouse', 'Bordeaux', 'Marseilles', 'Strasbourg', 'Berlin', 'Le', 'Versailles', 'Nantes', 'Brussels', 'Grenoble')\n",
      "('brother', 'sister', 'cousin', 'brothers', 'brother-in-law', 'uncle', 'nephew', 'father', 'son', 'sister-in-law', 'aunt', 'sisters', 'daughter', 'niece', 'dad', 'cousins', 'Brother', 'mother', 'siblings', 'grandfather')\n"
     ]
    }
   ],
   "source": [
    "def get_k_nearest_neighbors(vec, k):\n",
    "  return list(zip(*sorted(list(map(lambda key: (np.linalg.norm(vec - vecs[key]), key), vecs.keys())))))[1][:k]\n",
    "\n",
    "print(get_k_nearest_neighbors(vecs['Paris'], 20))\n",
    "print(get_k_nearest_neighbors(vecs['brother'], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['Paris'] - vecs['France'] + vecs['Germany'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['brother'] - vecs['man'] + vecs['woman'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_k_nearest_neighbors(vecs['king'] - vecs['man'] + vecs['woman'], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', categories = categories, shuffle = True, random_state = 42)\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories = categories,shuffle = True, random_state = 42)\n",
    "X_train = twenty_train.data\n",
    "y_train = twenty_train.target\n",
    "X_test = twenty_test.data\n",
    "y_test = twenty_test.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = load_vectors('crawl-300d-2M.vec', 2000000)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "logreg = LogisticRegression(solver = 'liblinear', multi_class = 'ovr', random_state = 1)\n",
    "\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred = logreg.predict(X_test_tfidf)\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 300, random_state = 1)\n",
    "X_train_tfidf_pca = pca.fit_transform(X_train_tfidf.todense())\n",
    "X_test_tfidf_pca = pca.transform(X_test_tfidf.todense())\n",
    "\n",
    "logreg.fit(X_train_tfidf_pca, y_train)\n",
    "y_pred_pca = logreg.predict(X_test_tfidf_pca)\n",
    "print(accuracy_score(y_test, y_pred_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "zero = sum(vecs.values()) / len(vecs)\n",
    "def text2vec(text):\n",
    "  words = text.split()\n",
    "  return sum(list(map(lambda w: np.array(list(vecs.get(w, zero))), words))) / len(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = list(map(lambda text: text2vec(text), X_train))\n",
    "X_test_vec = list(map(lambda text: text2vec(text), X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_vec, y_train)\n",
    "y_pred_vec = logreg.predict(X_test_vec)\n",
    "print(accuracy_score(y_test, y_pred_vec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "vocab = np.zeros((len(tfidf.vocabulary_.keys()), dim))\n",
    "for key in tqdm(tfidf.vocabulary_.keys()):\n",
    "  vocab[tfidf.vocabulary_[key]] = vecs.get(key, zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_weighted = X_train_tfidf.dot(vocab)\n",
    "X_test_weighted = X_test_tfidf.dot(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_weighted, y_train)\n",
    "y_pred_weighted = logreg.predict(X_test_weighted)\n",
    "print(accuracy_score(y_test, y_pred_weighted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('imdb_master.csv', encoding = 'latin-1')['review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = list(map(lambda text: text.split(), reviews))\n",
    "model = Word2Vec(sentences, min_count = 1, seed = 1, workers = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(wv.similar_by_word('Paris'))\n",
    "print()\n",
    "print(wv.similar_by_word('brother'))\n",
    "print()\n",
    "print(wv.similar_by_vector(wv['king'] - wv['man'] + wv['woman'], 1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "919eb0904b72c37e8d6c7e3b2f7b6f162c89cafcc297fba09b2d10c79c52f5eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
